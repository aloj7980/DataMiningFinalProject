{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Features\n",
      "PCA\n",
      "LASSO\n",
      "search complete\n",
      "search complete\n",
      "search complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import dask\n",
    "import dask.threaded\n",
    "from dask import delayed\n",
    "\n",
    "# Load your feature datasets\n",
    "pca = pd.read_csv(\"PCA_features.csv\")\n",
    "mf = pd.read_csv(\"manual_features.csv\")\n",
    "lasso = pd.read_csv(\"LASSO_features.csv\")\n",
    "res = pd.read_csv(\"response.csv\")\n",
    "\n",
    "# Define a list of feature datasets\n",
    "feature_datasets = [(\"PCA\", pca), (\"Manual Features\", mf), (\"LASSO\", lasso)]\n",
    "\n",
    "# Load the response variable\n",
    "y = res['avg_salary']\n",
    "\n",
    "# Initialize an empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define a delayed function for the randomized search\n",
    "@delayed\n",
    "def perform_randomized_search(dataset_name, data, y):\n",
    "    print(dataset_name)\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the XGBoost Regressor\n",
    "    xgb_regressor = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Define hyperparameter search space for n_estimators\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(1, 501),\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform RandomizedSearchCV to find the best hyperparameters\n",
    "    random_search = RandomizedSearchCV(xgb_regressor, param_distributions=param_dist, n_iter=100, \n",
    "                                       scoring='neg_mean_squared_error', cv=kf, random_state=42)\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "    \n",
    "    print(\"search complete\")\n",
    "    \n",
    "    # Calculate cross-validation RMSE and MAE scores\n",
    "    cv_rmse_scores = np.sqrt(-cross_val_score(best_xgb_model, data, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "    cv_mae_scores = -cross_val_score(best_xgb_model, data, y, cv=kf, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = best_xgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE on the test set\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"Best Hyperparameters\": random_search.best_params_,\n",
    "        \"Cross-Validation RMSE Scores\": cv_rmse_scores,\n",
    "        \"Mean Cross-Validation RMSE\": cv_rmse_scores.mean(),\n",
    "        \"Cross-Validation MAE Scores\": cv_mae_scores,\n",
    "        \"Mean Cross-Validation MAE\": cv_mae_scores.mean(),\n",
    "        \"RMSE on Test Set\": rmse,\n",
    "        \"MAE on Test Set\": mae\n",
    "    }\n",
    "\n",
    "# Create a list of delayed tasks for each feature dataset\n",
    "delayed_tasks = [perform_randomized_search(name, data, y) for name, data in feature_datasets]\n",
    "\n",
    "# Compute the delayed tasks in parallel\n",
    "results_list = dask.compute(*delayed_tasks, scheduler='threads')\n",
    "\n",
    "# Store results in the dictionary\n",
    "for result in results_list:\n",
    "    results[result[\"dataset_name\"]] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'dataset_name': 'PCA', 'Best Hyperparameters': {'n_estimators': 72}, 'Cross-Validation RMSE Scores': array([20.62066905, 17.58010469, 18.0952271 , 23.32811401, 17.61999003,\n",
      "       11.62401388, 25.7467744 , 15.54170888, 20.81377756, 21.19633134]), 'Mean Cross-Validation RMSE': 19.21667109435179, 'Cross-Validation MAE Scores': array([ 9.39599948,  7.62551503,  8.11597783,  9.07770983,  8.5073378 ,\n",
      "        5.05743823, 11.26819286,  6.78481664, 11.36541387, 10.64159922]), 'Mean Cross-Validation MAE': 8.784000078922995, 'RMSE on Test Set': 19.320333630669825, 'MAE on Test Set': 9.25872958906545}, {'dataset_name': 'Manual Features', 'Best Hyperparameters': {'n_estimators': 88}, 'Cross-Validation RMSE Scores': array([17.64179262, 16.14933994, 22.35414787, 22.9720448 , 17.42032474,\n",
      "       11.32339084, 21.54105668, 14.72262454, 21.40249777, 17.27745164]), 'Mean Cross-Validation RMSE': 18.28046714267173, 'Cross-Validation MAE Scores': array([ 9.11335108,  7.37094236, 12.54637538,  9.33997912,  8.82711454,\n",
      "        6.51497086, 11.07313343,  7.03730556, 10.93304541,  9.88322209]), 'Mean Cross-Validation MAE': 9.263943983438853, 'RMSE on Test Set': 17.27706516776784, 'MAE on Test Set': 9.196953651889059}, {'dataset_name': 'LASSO', 'Best Hyperparameters': {'n_estimators': 175}, 'Cross-Validation RMSE Scores': array([17.97628253, 23.72174808, 20.77788288, 18.57891231, 17.42820498,\n",
      "       12.0873144 , 25.13257894, 15.29565164, 16.94487949, 15.87041504]), 'Mean Cross-Validation RMSE': 18.381387030992396, 'Cross-Validation MAE Scores': array([10.4017321 , 10.16733861,  9.67617978,  8.38928245,  8.90414215,\n",
      "        6.14923034, 11.68332385,  6.62166175,  8.66911218,  8.62362101]), 'Mean Cross-Validation MAE': 8.928562422193924, 'RMSE on Test Set': 20.895616552867107, 'MAE on Test Set': 9.93780216754683})\n"
     ]
    }
   ],
   "source": [
    "print(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features Ranked by Importance for PCA:\n",
      "is_senior: 0.1692\n",
      "Principal_Component_3: 0.1256\n",
      "python_yn: 0.1036\n",
      "Principal_Component_17: 0.0548\n",
      "Principal_Component_2: 0.0480\n",
      "Principal_Component_9: 0.0468\n",
      "employer_provided: 0.0448\n",
      "Principal_Component_15: 0.0313\n",
      "Principal_Component_14: 0.0287\n",
      "Principal_Component_16: 0.0275\n",
      "\n",
      "\n",
      "Top 10 Features Ranked by Importance for Manual Features:\n",
      "hourly: 0.2804\n",
      "job_title_analyst: 0.1121\n",
      "job_title_director: 0.0940\n",
      "Sector_Oil, Gas, Energy & Utilities: 0.0642\n",
      "Ownership_Nonprofit Organization: 0.0568\n",
      "employer_provided: 0.0563\n",
      "is_senior: 0.0545\n",
      "python_yn: 0.0445\n",
      "Sector_Health Care: 0.0322\n",
      "Sector_Finance: 0.0239\n",
      "\n",
      "\n",
      "Top 10 Features Ranked by Importance for LASSO Features:\n",
      "Industry_Health Care Services & Hospitals: 0.1292\n",
      "Headquarters_Mountain View, CA: 0.1227\n",
      "Job Title_Data Science Manager: 0.0775\n",
      "job_simp_analyst: 0.0735\n",
      "job_simp_director: 0.0655\n",
      "job_state_CA: 0.0400\n",
      "Company Name_Takeda Pharmaceuticals\n",
      "3.7: 0.0331\n",
      "is_senior: 0.0306\n",
      "Job Title_Director II, Data Science - GRM Actuarial: 0.0258\n",
      "Size_Unknown: 0.0244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define a function to print the top n features by importance\n",
    "def print_top_features(importances, feature_names, top_n=10):\n",
    "    feature_importance = list(zip(feature_names, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in feature_importance[:top_n]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Define hyperparameters for each dataset\n",
    "hyperparameters = {\n",
    "    'PCA': {'n_estimators': 72},\n",
    "    'Manual Features': {'n_estimators': 88},\n",
    "    'LASSO Features': {'n_estimators': 175}\n",
    "}\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset_name, data in [(\"PCA\", pca), (\"Manual Features\", mf), (\"LASSO Features\", lasso)]:\n",
    "    # Split the data\n",
    "    X_train, _, y_train, _ = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create an XGBRegressor with the specified hyperparameters\n",
    "    xgb_regressor = XGBRegressor(\n",
    "        n_estimators=hyperparameters[dataset_name]['n_estimators'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the XGBoost model\n",
    "    xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Print the top 10 features by importance\n",
    "    print(f\"Top 10 Features Ranked by Importance for {dataset_name}:\")\n",
    "    print_top_features(xgb_regressor.feature_importances_, data.columns, top_n=10)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
