{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCAManual Features\n",
      "LASSO\n",
      "\n",
      "search complete\n",
      "search complete\n",
      "search complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "import dask\n",
    "import dask.threaded\n",
    "from dask import delayed\n",
    "\n",
    "# Load your feature datasets\n",
    "pca = pd.read_csv(\"PCA_features.csv\")\n",
    "mf = pd.read_csv(\"manual_features.csv\")\n",
    "lasso = pd.read_csv(\"LASSO_features.csv\")\n",
    "res = pd.read_csv(\"response.csv\")\n",
    "\n",
    "# Define a list of feature datasets\n",
    "feature_datasets = [(\"PCA\", pca), (\"Manual Features\", mf), (\"LASSO\", lasso)]\n",
    "\n",
    "# Load the response variable\n",
    "y = res['avg_salary']\n",
    "\n",
    "# Initialize an empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define a delayed function for the randomized search\n",
    "@delayed\n",
    "def perform_randomized_search(dataset_name, data, y):\n",
    "    print(dataset_name)\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the base regressor and AdaBoostRegressor\n",
    "    base_regressor = DecisionTreeRegressor()\n",
    "    adaboost_regressor = AdaBoostRegressor(base_regressor, random_state=42)\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(1, 501),\n",
    "        'learning_rate': uniform(0, 1),\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform RandomizedSearchCV to find the best hyperparameters\n",
    "    random_search = RandomizedSearchCV(adaboost_regressor, param_distributions=param_dist, n_iter=100, \n",
    "                                       scoring='neg_mean_squared_error', cv=kf, random_state=42)\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_adaboost_model = random_search.best_estimator_\n",
    "    \n",
    "    print(\"search complete\")\n",
    "    \n",
    "    # Calculate cross-validation RMSE and MAE scores\n",
    "    cv_rmse_scores = np.sqrt(-cross_val_score(best_adaboost_model, data, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "    cv_mae_scores = -cross_val_score(best_adaboost_model, data, y, cv=kf, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = best_adaboost_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE on the test set\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"Best Hyperparameters\": random_search.best_params_,\n",
    "        \"Cross-Validation RMSE Scores\": cv_rmse_scores,\n",
    "        \"Mean Cross-Validation RMSE\": cv_rmse_scores.mean(),\n",
    "        \"Cross-Validation MAE Scores\": cv_mae_scores,\n",
    "        \"Mean Cross-Validation MAE\": cv_mae_scores.mean(),\n",
    "        \"RMSE on Test Set\": rmse,\n",
    "        \"MAE on Test Set\": mae\n",
    "    }\n",
    "\n",
    "# Create a list of delayed tasks for each feature dataset\n",
    "delayed_tasks = [perform_randomized_search(name, data, y) for name, data in feature_datasets]\n",
    "\n",
    "# Compute the delayed tasks in parallel\n",
    "results_list = dask.compute(*delayed_tasks, scheduler='threads')\n",
    "\n",
    "# Store results in the dictionary\n",
    "for result in results_list:\n",
    "    results[result[\"dataset_name\"]] = result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PCA\n",
      "dataset_name: PCA\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.8036720768991145\n",
      "  n_estimators: 44\n",
      "Cross-Validation RMSE Scores: [18.810901094843917, 19.79292802998081, 21.305452290371527, 21.837467801922458, 16.356356727749777, 9.288360923685548, 20.737581761075646, 16.264702288715295, 19.45837553287474, 18.338686769570838]\n",
      "Mean Cross-Validation RMSE: 18.21908132207906\n",
      "Cross-Validation MAE Scores: [8.42, 8.093333333333334, 9.304054054054054, 7.547297297297297, 7.4391891891891895, 3.9932432432432434, 8.864864864864865, 6.162162162162162, 10.04054054054054, 8.641891891891891]\n",
      "Mean Cross-Validation MAE: 7.850657657657658\n",
      "RMSE on Test Set: 19.824650779357754\n",
      "MAE on Test Set: 9.10738255033557\n",
      "\n",
      "\n",
      "Dataset: Manual Features\n",
      "dataset_name: Manual Features\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.9717120953891037\n",
      "  n_estimators: 131\n",
      "Cross-Validation RMSE Scores: [16.202263216394595, 18.11868280716528, 19.671027536057295, 23.68936070920469, 14.752576504242013, 10.813530063417424, 20.79890849526952, 13.577853589875982, 17.772967630037996, 14.141180041742379]\n",
      "Mean Cross-Validation RMSE: 16.953835059340715\n",
      "Cross-Validation MAE Scores: [7.586666666666667, 6.266666666666667, 9.074324324324325, 8.43918918918919, 6.087837837837838, 5.6891891891891895, 9.716216216216216, 5.0, 9.391891891891891, 7.54054054054054]\n",
      "Mean Cross-Validation MAE: 7.479252252252252\n",
      "RMSE on Test Set: 16.184392597758784\n",
      "MAE on Test Set: 6.909395973154362\n",
      "\n",
      "\n",
      "Dataset: LASSO\n",
      "dataset_name: LASSO\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.7106628896857874\n",
      "  n_estimators: 346\n",
      "Cross-Validation RMSE Scores: [16.20313184176076, 21.289884139343425, 16.81831618461229, 22.016421670754024, 15.92554329547303, 12.316353663628192, 23.450291349248946, 16.041203471799346, 19.142140594681322, 13.24636649847689]\n",
      "Mean Cross-Validation RMSE: 17.644965270977828\n",
      "Cross-Validation MAE Scores: [8.977777777777778, 9.65, 8.510082347972974, 9.122648880143528, 8.378157371941285, 6.344594594594595, 11.158593797026425, 7.043372747747747, 9.862612612612612, 7.352787077787078]\n",
      "Mean Cross-Validation MAE: 8.640062720760403\n",
      "RMSE on Test Set: 18.620809633679354\n",
      "MAE on Test Set: 9.137236715520757\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_dict in results.items():\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for key, value in dataset_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"{key}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                if isinstance(sub_value, (list, np.ndarray)):\n",
    "                    print(f\"  {sub_key}: {sub_value.tolist()}\")\n",
    "                else:\n",
    "                    print(f\"  {sub_key}: {sub_value}\")\n",
    "        elif isinstance(value, (list, np.ndarray)):\n",
    "            print(f\"{key}: {value.tolist()}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features Ranked by Importance for PCA:\n",
      "Principal_Component_3: 0.1243\n",
      "is_senior: 0.0843\n",
      "Principal_Component_9: 0.0800\n",
      "Principal_Component_17: 0.0600\n",
      "Principal_Component_16: 0.0528\n",
      "Principal_Component_10: 0.0439\n",
      "Principal_Component_12: 0.0421\n",
      "Principal_Component_11: 0.0365\n",
      "Principal_Component_15: 0.0363\n",
      "python_yn: 0.0355\n",
      "\n",
      "\n",
      "Top 10 Features Ranked by Importance for Manual Features:\n",
      "Longitude: 0.0827\n",
      "is_senior: 0.0779\n",
      "job_title_analyst: 0.0702\n",
      "hourly: 0.0673\n",
      "desc_len: 0.0656\n",
      "job_title_director: 0.0600\n",
      "Rating: 0.0582\n",
      "HQ_Longitude: 0.0580\n",
      "age: 0.0507\n",
      "Log_Population: 0.0477\n",
      "\n",
      "\n",
      "Top 10 Features Ranked by Importance for LASSO Features:\n",
      "Rating: 0.0967\n",
      "python_yn: 0.0802\n",
      "job_state_CA: 0.0693\n",
      "is_senior: 0.0680\n",
      "job_simp_director: 0.0605\n",
      "job_simp_analyst: 0.0453\n",
      "Job Title_Data Science Manager: 0.0399\n",
      "Location_San Francisco, CA: 0.0296\n",
      "Revenue_$5 to $10 million (USD): 0.0212\n",
      "Competitors_-1: 0.0210\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to print the top n features by importance\n",
    "def print_top_features(importances, feature_names, top_n=10):\n",
    "    feature_importance = list(zip(feature_names, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in feature_importance[:top_n]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Initialize the base regressor (DecisionTreeRegressor) for AdaBoost\n",
    "base_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Define hyperparameters for each dataset\n",
    "hyperparameters = {\n",
    "    'PCA': {'n_estimators': 44, 'learning_rate': 0.8036720768991145},\n",
    "    'Manual Features': {'n_estimators': 131, 'learning_rate': 0.9717120953891037},\n",
    "    'LASSO Features': {'n_estimators': 346, 'learning_rate': 0.7106628896857874}\n",
    "}\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset_name, data in [(\"PCA\", pca), (\"Manual Features\", mf), (\"LASSO Features\", lasso)]:\n",
    "    # Split the data\n",
    "    X_train, _, y_train, _ = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create an AdaBoostRegressor with the specified hyperparameters\n",
    "    ada_boost_regressor = AdaBoostRegressor(\n",
    "        base_regressor,\n",
    "        n_estimators=hyperparameters[dataset_name]['n_estimators'],\n",
    "        learning_rate=hyperparameters[dataset_name]['learning_rate'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the AdaBoost model\n",
    "    ada_boost_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Print the top 10 features by importance\n",
    "    print(f\"Top 10 Features Ranked by Importance for {dataset_name}:\")\n",
    "    print_top_features(ada_boost_regressor.feature_importances_, data.columns, top_n=10)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
